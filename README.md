# llm-infer
Benchmark and identify the best ways to speedup LLM inference.

Resources

[llm-hacker](https://github.com/fastai/lm-hackers/blob/main/lm-hackers.ipynb)

[coursera-llm](https://github.com/Ryota-Kawamura/Generative-AI-with-LLMs/tree/main)

[fine-tune-mistral](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Mistral)

[prompt-engineering-guide](https://www.promptingguide.ai/) : This has mistral specific details.

[Coursera prompt-engineering course](https://github.com/ksm26/chatGPT-Prompt-Engineering-for-Developers/tree/main)

[another prompt engineering course](https://github.com/mikeffendii/Building-Systems-with-the-ChatGPT-API)

[mistral finetune blog](https://medium.com/@parikshitsaikia1619/mistral-mastery-fine-tuning-fast-inference-guide-62e163198b06)


For structuring experimets(mlops): https://github.com/vin136/MLOPS

Fine tuning llms

https://www.youtube.com/playlist?list=PL23FjyM69j92o_j5JFH9sNlbhCx4n0ZYh

https://wandb.ai/johnowhitaker/llmemo/reports/Can-LLMs-learn-from-a-single-example---Vmlldzo1MjQ2MDYy

https://johnowhitaker.dev/

Hugging face blog/references

[generation strategies](https://huggingface.co/docs/transformers/en/generation_strategies)
[chat-template](https://huggingface.co/docs/transformers/en/chat_templating)
[prompting](https://huggingface.co/docs/transformers/en/tasks/prompting)

End-point detection

[alexa paper](https://arxiv.org/pdf/2401.08916.pdf)

basically, use an ML model to detect end-of-command.

Practical solutions:

[vad - detect end point](https://medium.com/axinc-ai/silerovad-machine-learning-model-to-detect-speech-segments-e99722c0dd41)
[faster whisper](https://github.com/SYSTRAN/faster-whisper), [whisper-live](https://github.com/collabora/WhisperLive)

